{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d76bf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the access token for the API. Send us an email at info@exploitability.app if you need one.\n",
    "%env API_ACCESS_TOKEN=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7e49eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the date when the scores are computed\n",
    "SCORES_DATE = \"2021-10-10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581d008b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reference scores:\n",
    "SCORES_FOLDER = \"scores_latest_ref\"\n",
    "\n",
    "# To use the API, uncomment the following line. This is the folder where the scores will be cached after they are downloaded from the API\n",
    "#SCORES_FOLDER = \"scores_latest_download\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a78b87",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import pandas as pd\n",
    "from dateutil.parser import parse as date_parser\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "from bokeh.models import Legend\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.plotting import figure,show\n",
    "from bokeh.palettes import Colorblind as palette\n",
    "import ee_api_client as api_client\n",
    "output_notebook()\n",
    "\n",
    "scores_date = date_parser(SCORES_DATE)\n",
    "\n",
    "BASE_DATA_FOLDER = \"./data/\"\n",
    "BASE_SCORES_FOLDER = os.path.join(BASE_DATA_FOLDER,SCORES_FOLDER)\n",
    "\n",
    "# delete the cache folder if it already exists; this will force scores to be redownloaded from the API\n",
    "if \"ref\" not in BASE_SCORES_FOLDER and os.path.exists(BASE_SCORES_FOLDER):\n",
    "    shutil.rmtree(BASE_SCORES_FOLDER)\n",
    "\n",
    "if not os.path.exists(BASE_SCORES_FOLDER):\n",
    "    os.mkdir(BASE_SCORES_FOLDER)\n",
    "\n",
    "# read set of vulnerabilities that are known to be exploited\n",
    "fi = open(os.path.join(BASE_DATA_FOLDER,'exploit_labels.json'),'r')\n",
    "exploited_cveids = set(json.loads(fi.read()))\n",
    "\n",
    "'''\n",
    "Read all predictions from from one model on one day.\n",
    "If the file does not exist, download these scores from the API, and assign the labels to each prediction for performance evalation.\n",
    "Params:\n",
    "model - name of the model used to output the scores\n",
    "date - date when the scores wcomputedere \n",
    "'''\n",
    "def download_or_read_scores(model,date):\n",
    "\n",
    "    # compose the model name that will be used to identify the provenance of each score \n",
    "    scores_batch = 'latest_%s_%s' % (model,date.strftime('%Y-%m-%d'))\n",
    "    # define cache filename to save results to\n",
    "    scores_batch_file = os.path.join(BASE_SCORES_FOLDER,'%s.json' % (scores_batch))\n",
    "\n",
    "\n",
    "    if not os.path.exists(scores_batch_file):\n",
    "        # download the results using the API if the cache file does not exist\n",
    "        scores_list = api_client.download_daily_scores(date=date,model=model)\n",
    "\n",
    "        for score in scores_list:\n",
    "            score['label'] = score['cveid'] in exploited_cveids\n",
    "\n",
    "        fo = open(scores_batch_file,'w')\n",
    "        fo.write(json.dumps(scores_list))\n",
    "        fo.close()\n",
    "\n",
    "    # read the results from cache file\n",
    "    fi = open(scores_batch_file,'r')\n",
    "    scores_list = json.loads(fi.read())\n",
    "\n",
    "    if len(scores_list) == 0:\n",
    "        return pd.DataFrame(columns=[\"cveid\",\"label\",\"score\"]).set_index('cveid')\n",
    "\n",
    "    result = pd.DataFrame(scores_list).set_index('cveid')\n",
    "\n",
    "    return result\n",
    "\n",
    "'''\n",
    "Plot one result on both figures.\n",
    "Params:\n",
    "binsettings - identifier for the series\n",
    "p1 - figure for the TPR-FPR plot\n",
    "p2 - figure for the Precision-Recall plot\n",
    "fpr - points to plot for FPR\n",
    "tpr - points to plot for TPR\n",
    "auc_roc - AUC value for TPR-FPR plot\n",
    "precision - points to plot for Precision\n",
    "recall - points to plot for Recall\n",
    "auc_rp - AUC value for the Precision-Recall plot\n",
    "color - color to use for the series\n",
    "'''\n",
    "def do_plot(binsettings, p1, p2, fpr, tpr, auc_roc, precision, recall, auc_pr, color):\n",
    "    if binsettings in ['baseline_usenix15']:\n",
    "        p1.line(fpr, tpr, line_width=7,color=color,line_dash='solid',legend_label=r'%s (AUC = %0.2f)' % ('Baseline: Social Media Clf',auc_roc))\n",
    "        p2.line(recall, precision, line_width=7,color=color,line_dash='solid',legend_label=r'%s (AUC = %0.2f)' % ('Baseline: Social Media Clf',auc_pr))\n",
    "        \n",
    "    if binsettings in ['all','epss']:\n",
    "        if binsettings == 'all':\n",
    "            featurename = 'EE'\n",
    "        elif binsettings == 'epss':\n",
    "            featurename = 'Baseline: EPSS'\n",
    "        p1.line(fpr, tpr, line_width=10,color=color,line_dash='solid',legend_label=r'%s (AUC = %0.2f)' % (featurename, auc_roc))\n",
    "        p2.line(recall, precision, line_width=10,color=color,line_dash='solid',legend_label=r'%s (AUC = %0.2f)' % (featurename, auc_pr))\n",
    " \n",
    "'''\n",
    "Read all prediction scores for a model on a date, their labels, and plot the performance.\n",
    "Params:\n",
    "p1 - figure for the TPR-FPR plot\n",
    "p2 - figure for the Precision-Recall plot\n",
    "model - name of the model used to output the scores\n",
    "date - date when the scores wcomputedere \n",
    "'''\n",
    "def plot_result(p1, p2, model, date):\n",
    "    \n",
    "\n",
    "    # read all scores into a single DataFrame\n",
    "    results = pd.DataFrame(columns =['cveid','score','label']).set_index('cveid')\n",
    "    r = download_or_read_scores(model, date)\n",
    "    results = pd.concat([results,r])\n",
    "\n",
    "    # compute performance metrics for all scores and labels\n",
    "    cveids = list(results.index.values)\n",
    "    predictions0 = list(results['score'].values)\n",
    "    labels0 = list(results['label'].values)\n",
    "\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    for idxp,_ in enumerate(cveids):\n",
    "        predictions.append(predictions0[idxp])\n",
    "        labels.append(labels0[idxp])\n",
    "\n",
    "    print ('# instances:',len(predictions))\n",
    "    print ('# exploited:',sum(labels))\n",
    "\n",
    "    if len(labels) == 0:\n",
    "        return\n",
    "\n",
    "    labels = list(map(lambda e: 1 if e == 1 else 0, labels))\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(labels, predictions, pos_label=1)\n",
    "    auc_roc = auc(fpr, tpr)\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(labels, predictions, pos_label=1) \n",
    "    auc_pr = auc(recall, precision)\n",
    "\n",
    "    # plot the obtained performance \n",
    "    do_plot('all',p1,p2, fpr, tpr, auc_roc, precision, recall, auc_pr, next(colors))        \n",
    "\n",
    "\n",
    "\n",
    "# define the Prcision-Recall and TPR-FPR figures that are used for plotting all results\n",
    "p1 = figure()\n",
    "p1.yaxis.axis_label = 'True Positive Rate'\n",
    "p1.xaxis.axis_label = 'False Positive Rate'\n",
    "\n",
    "p2 = figure()\n",
    "p2.yaxis.axis_label = 'Precision'\n",
    "p2.xaxis.axis_label = 'Recall'\n",
    "\n",
    "p1.add_layout(Legend())\n",
    "p2.add_layout(Legend())\n",
    "\n",
    "FONT_SIZE2 = \"16pt\"\n",
    "\n",
    "p1.legend.location = \"bottom_right\"\n",
    "p1.legend.label_text_font_size = FONT_SIZE2\n",
    "p1.y_range.start = -0.01\n",
    "p1.y_range.end = 1.01\n",
    "p1.x_range.start = -0.01\n",
    "p1.x_range.end = 1.01\n",
    "p1.xaxis.major_label_text_font_size = FONT_SIZE2\n",
    "p1.xaxis.axis_label_text_font_size = FONT_SIZE2\n",
    "p1.yaxis.major_label_text_font_size = FONT_SIZE2\n",
    "p1.yaxis.axis_label_text_font_size = FONT_SIZE2\n",
    "\n",
    "p2.legend.location = \"bottom_left\"\n",
    "p2.legend.label_text_font_size = FONT_SIZE2\n",
    "p2.legend.background_fill_alpha = 0.2\n",
    "p2.y_range.start = -0.01\n",
    "p2.y_range.end = 1.01\n",
    "p2.x_range.start = -0.01\n",
    "p2.x_range.end = 1.01\n",
    "p2.xaxis.major_label_text_font_size = FONT_SIZE2\n",
    "p2.xaxis.axis_label_text_font_size = FONT_SIZE2\n",
    "p2.yaxis.major_label_text_font_size = FONT_SIZE2\n",
    "p2.yaxis.axis_label_text_font_size = FONT_SIZE2\n",
    "\n",
    "p1.output_backend = \"svg\"\n",
    "p2.output_backend = \"svg\"\n",
    "\n",
    "colors = iter(palette[8])\n",
    "\n",
    "\n",
    "# plotting the performance using all the scores computed on a single date\n",
    "model = 'prod3_2021_07_21'\n",
    "plot_result(p1,p2,model=model,date=scores_date)\n",
    "\n",
    "# exporting plots to file\n",
    "show(p1)\n",
    "show(p2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214f36a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
