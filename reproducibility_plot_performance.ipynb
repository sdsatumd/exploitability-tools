{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcb09e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the access token for the API. Send us an email at info@exploitability.app if you need one.\n",
    "%env API_ACCESS_TOKEN=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e68f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reference scores:\n",
    "SCORES_FOLDER = \"scores_reproducibility_ref\"\n",
    "\n",
    "# To use the API, uncomment the following line. This is the folder where the scores will be cached after they are downloaded from the API\n",
    "#SCORES_FOLDER = \"scores_reproducibility_download\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7281dd9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import pandas as pd\n",
    "from dateutil.parser import parse as date_parser\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "from bokeh.models import Legend\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.plotting import figure,show\n",
    "from bokeh.palettes import Colorblind as palette\n",
    "import ee_api_client as api_client\n",
    "output_notebook()\n",
    "\n",
    "PAPER_SCORES_DATE = date_parser('2021-03-04')\n",
    "\n",
    "\n",
    "BASE_DATA_FOLDER = \"./data/\"\n",
    "BASE_SCORES_FOLDER = os.path.join(BASE_DATA_FOLDER,SCORES_FOLDER)\n",
    "\n",
    "# delete the cache folder if it already exists; this will force scores to be redownloaded from the API\n",
    "if \"ref\" not in BASE_SCORES_FOLDER and os.path.exists(BASE_SCORES_FOLDER):\n",
    "    shutil.rmtree(BASE_SCORES_FOLDER)\n",
    "\n",
    "if not os.path.exists(BASE_SCORES_FOLDER):\n",
    "    os.mkdir(BASE_SCORES_FOLDER)\n",
    "\n",
    "\n",
    "# read performance scores for baselines\n",
    "dict_results_baselines = json.loads(open(os.path.join(BASE_DATA_FOLDER,'dict_results_baselines_classification_20210304.json')).read())\n",
    "\n",
    "# read set of vulnerabilities that are known to be exploited\n",
    "fi = open(os.path.join(BASE_DATA_FOLDER,'exploit_labels.json'),'r')\n",
    "exploited_cveids = set(json.loads(fi.read()))\n",
    "\n",
    "'''\n",
    "Read all predictions from from one model on one day (all predictions from one model which uses a particular set of featurs, and for which the predictions are computed at a certain number of days after the vulnerability is disclosed).\n",
    "If the file does not exist, download these scores from the API, and assign the labels to each prediction for performance evalation.\n",
    "Params:\n",
    "name_experiment - model identifier; used to identify the origin of the scores\n",
    "fs - feature set identifier; used to identify the origin of the scores\n",
    "b - fold number; used to identify the origin of the scores\n",
    "days_from_disclosure - prediction delay; used to identify the origin of the scores\n",
    "'''\n",
    "def download_or_read_scores(name_experiment,fs,b,days_from_disclosure):\n",
    "\n",
    "    # compose the model name that will be used to identify the provenance of each score \n",
    "    scores_batch = 'paper_%s_%s_%d_%d' % (name_experiment, fs, b, days_from_disclosure)\n",
    "    # define cache filename to save results to\n",
    "    scores_batch_file = os.path.join(BASE_SCORES_FOLDER,'%s.json' % (scores_batch))\n",
    "\n",
    "\n",
    "    if not os.path.exists(scores_batch_file):\n",
    "        # download the results using the API if the cache file does not exist\n",
    "        scores_list = api_client.download_daily_scores(date=PAPER_SCORES_DATE,model=scores_batch)\n",
    "\n",
    "        for score in scores_list:\n",
    "            score['label'] = score['cveid'] in exploited_cveids\n",
    "\n",
    "        fo = open(scores_batch_file,'w')\n",
    "        fo.write(json.dumps(scores_list))\n",
    "        fo.close()\n",
    "\n",
    "    # read the results from cache file\n",
    "    fi = open(scores_batch_file,'r')\n",
    "    scores_list = json.loads(fi.read())\n",
    "\n",
    "    if len(scores_list) == 0:\n",
    "        return pd.DataFrame(columns=[\"cveid\",\"label\",\"score\"]).set_index('cveid')\n",
    "\n",
    "    return pd.DataFrame(scores_list).set_index('cveid')\n",
    "\n",
    "'''\n",
    "Plot one result on both figures.\n",
    "Params:\n",
    "binsettings - identifier for the series\n",
    "p1 - figure for the TPR-FPR plot\n",
    "p2 - figure for the Precision-Recall plot\n",
    "fpr - points to plot for FPR\n",
    "tpr - points to plot for TPR\n",
    "auc_roc - AUC value for TPR-FPR plot\n",
    "precision - points to plot for Precision\n",
    "recall - points to plot for Recall\n",
    "auc_rp - AUC value for the Precision-Recall plot\n",
    "color - color to use for the series\n",
    "'''\n",
    "def do_plot(binsettings, p1, p2, fpr, tpr, auc_roc, precision, recall, auc_pr, color):\n",
    "    if binsettings in ['baseline_usenix15']:\n",
    "        p1.line(fpr, tpr, line_width=7,color=color,line_dash='solid',legend_label=r'%s (AUC = %0.2f)' % ('Baseline: Social Media Clf',auc_roc))\n",
    "        p2.line(recall, precision, line_width=7,color=color,line_dash='solid',legend_label=r'%s (AUC = %0.2f)' % ('Baseline: Social Media Clf',auc_pr))\n",
    "        \n",
    "    if binsettings in ['all','epss']:\n",
    "        if binsettings == 'all':\n",
    "            featurename = 'EE'\n",
    "        elif binsettings == 'epss':\n",
    "            featurename = 'Baseline: EPSS'\n",
    "        p1.line(fpr, tpr, line_width=10,color=color,line_dash='solid',legend_label=r'%s (AUC = %0.2f)' % (featurename, auc_roc))\n",
    "        p2.line(recall, precision, line_width=10,color=color,line_dash='solid',legend_label=r'%s (AUC = %0.2f)' % (featurename, auc_pr))\n",
    " \n",
    "'''\n",
    "Read prediction scores and labels across multiple folds for one result, and plot one result.\n",
    "Params:\n",
    "p1 - figure for the TPR-FPR plot\n",
    "p2 - figure for the Precision-Recall plot\n",
    "name_experiment - model identifier; used to identify the origin of the scores\n",
    "fs - feature set identifier; used to identify the origin of the scores\n",
    "days_from_disclosure - prediction delay; used to identify the origin of the scores\n",
    "startidx - index for the first fold to include in results\n",
    "endidx - index for the last fold to include in results\n",
    "'''\n",
    "def plot_result(p1, p2, name_experiment, fs, days_from_disclosure=None, startidx=None, endidx=None):\n",
    "    \n",
    "    # define the evaluation folds that span the entire test set of the paper\n",
    "    YR_RANGE = [2010,2011,2012,2013,2014,2015,2016,2017,2018,2019]\n",
    "    dates_end_train_list = []\n",
    "    for YR in YR_RANGE:\n",
    "        dates_end_train_list.append(date_parser('%d-Jan-01' % YR))\n",
    "        dates_end_train_list.append(date_parser('%d-Jul-01' % YR))\n",
    "\n",
    "    folds = list(range(0,len(dates_end_train_list)))\n",
    "\n",
    "    # concatenate scores from all valid folds into a single DataFrame\n",
    "    results = pd.DataFrame(columns =['cveid','score','label']).set_index('cveid')\n",
    "    for b in folds:\n",
    "        if endidx is not None and b > endidx:\n",
    "            continue\n",
    "        if startidx is not None and b < startidx:\n",
    "            continue\n",
    "\n",
    "        r = download_or_read_scores(name_experiment,fs,b,days_from_disclosure)\n",
    "\n",
    "        results = pd.concat([results,r])\n",
    "\n",
    "    # compute performance metrics for all scores and labels\n",
    "    cveids = list(results.index.values)\n",
    "    predictions0 = list(results['score'].values)\n",
    "    labels0 = list(results['label'].values)\n",
    "\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    for idxp,_ in enumerate(cveids):\n",
    "        predictions.append(predictions0[idxp])\n",
    "        labels.append(labels0[idxp])\n",
    "\n",
    "    print ('# instances:',len(predictions))\n",
    "    print ('# exploited:',sum(labels))\n",
    "\n",
    "    if len(labels) == 0:\n",
    "        return\n",
    "\n",
    "    labels = list(map(lambda e: 1 if e == 1 else 0, labels))\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(labels, predictions, pos_label=1)\n",
    "    auc_roc = auc(fpr, tpr)\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(labels, predictions, pos_label=1) \n",
    "    auc_pr = auc(recall, precision)\n",
    "\n",
    "    # plot the obtained performance \n",
    "    do_plot(fs,p1,p2, fpr, tpr, auc_roc, precision, recall, auc_pr, next(colors))        \n",
    "\n",
    "\n",
    "\n",
    "# define the Prcision-Recall and TPR-FPR figures that are used for plotting all results\n",
    "p1 = figure()\n",
    "p1.yaxis.axis_label = 'True Positive Rate'\n",
    "p1.xaxis.axis_label = 'False Positive Rate'\n",
    "\n",
    "p2 = figure()\n",
    "p2.yaxis.axis_label = 'Precision'\n",
    "p2.xaxis.axis_label = 'Recall'\n",
    "\n",
    "p1.add_layout(Legend())\n",
    "p2.add_layout(Legend())\n",
    "\n",
    "FONT_SIZE2 = \"16pt\"\n",
    "\n",
    "p1.legend.location = \"bottom_right\"\n",
    "p1.legend.label_text_font_size = FONT_SIZE2\n",
    "p1.y_range.start = -0.01\n",
    "p1.y_range.end = 1.01\n",
    "p1.x_range.start = -0.01\n",
    "p1.x_range.end = 1.01\n",
    "p1.xaxis.major_label_text_font_size = FONT_SIZE2\n",
    "p1.xaxis.axis_label_text_font_size = FONT_SIZE2\n",
    "p1.yaxis.major_label_text_font_size = FONT_SIZE2\n",
    "p1.yaxis.axis_label_text_font_size = FONT_SIZE2\n",
    "\n",
    "p2.legend.location = \"bottom_left\"\n",
    "p2.legend.label_text_font_size = FONT_SIZE2\n",
    "p2.legend.background_fill_alpha = 0.2\n",
    "p2.y_range.start = -0.01\n",
    "p2.y_range.end = 1.01\n",
    "p2.x_range.start = -0.01\n",
    "p2.x_range.end = 1.01\n",
    "p2.xaxis.major_label_text_font_size = FONT_SIZE2\n",
    "p2.xaxis.axis_label_text_font_size = FONT_SIZE2\n",
    "p2.yaxis.major_label_text_font_size = FONT_SIZE2\n",
    "p2.yaxis.axis_label_text_font_size = FONT_SIZE2\n",
    "\n",
    "p1.output_backend = \"svg\"\n",
    "p2.output_backend = \"svg\"\n",
    "\n",
    "colors = iter(palette[8])\n",
    "\n",
    "\n",
    "# index of the start fold for evaluation\n",
    "startidx = 0\n",
    "# index of the end fold for evaluation\n",
    "endidx = 18\n",
    "# how many days after disclosure the the score is computed after\n",
    "DAYS_FROM_DISCLOSURE_THRESHOLD_TEST = 30\n",
    "\n",
    "# plotting Expected Exploitability (EE)\n",
    "name_experiment = 'latest_gt13_v0'\n",
    "features = 'all'\n",
    "print ('Plotting experiment %s' % name_experiment)\n",
    "plot_result(p1,p2,name_experiment,features,days_from_disclosure=DAYS_FROM_DISCLOSURE_THRESHOLD_TEST,startidx=startidx,endidx=endidx)\n",
    "\n",
    "# plotting Baseline: EPSS\n",
    "name_experiment = 'baseline_gt13_v0_epss'\n",
    "features = 'epss'\n",
    "print ('Plotting experiment %s' % name_experiment)\n",
    "plot_result(p1,p2,name_experiment,features,days_from_disclosure=DAYS_FROM_DISCLOSURE_THRESHOLD_TEST,startidx=startidx,endidx=endidx)\n",
    "\n",
    "# plotting Baseline: Social Media Clf\n",
    "name_experiment = 'baseline_gt13_v0_baseline_usenix15'\n",
    "features = 'baseline_usenix15'\n",
    "print ('Plotting experiment %s' % name_experiment)\n",
    "plot_result(p1,p2,name_experiment,features,days_from_disclosure=DAYS_FROM_DISCLOSURE_THRESHOLD_TEST,startidx=10,endidx=endidx)\n",
    "\n",
    "# plotting Baseline: MS Exploitability\n",
    "msexp_p = dict_results_baselines['mse'][str(DAYS_FROM_DISCLOSURE_THRESHOLD_TEST)]['P']\n",
    "msexp_r = dict_results_baselines['mse'][str(DAYS_FROM_DISCLOSURE_THRESHOLD_TEST)]['R']    \n",
    "c = next(colors)\n",
    "p2.circle(msexp_r, msexp_p, line_width=10,color=c,legend_label=r'Baseline: MS Exploitability' )   \n",
    "p2.line(msexp_r, msexp_p, line_width=5,color=c,legend_label=r'Baseline: MS Exploitability' )    \n",
    "\n",
    "# plotting Baseline: RedHat Severity\n",
    "rhexp_p = dict_results_baselines['rh'][str(DAYS_FROM_DISCLOSURE_THRESHOLD_TEST)]['P']\n",
    "rhexp_r = dict_results_baselines['rh'][str(DAYS_FROM_DISCLOSURE_THRESHOLD_TEST)]['R']\n",
    "c = next(colors)\n",
    "p2.circle(rhexp_r, rhexp_p, line_width=10,color=c,legend_label=r'Baseline: RedHat Severity' )    \n",
    "p2.line(rhexp_r, rhexp_p, line_width=5,color=c,legend_label=r'Baseline: RedHat Severity' )       \n",
    "\n",
    "# plotting Baseline: CVSSv3 Exploitability\n",
    "cvss_exp_p = dict_results_baselines['cvssv3'][str(DAYS_FROM_DISCLOSURE_THRESHOLD_TEST)]['P']\n",
    "cvss_exp_r = dict_results_baselines['cvssv3'][str(DAYS_FROM_DISCLOSURE_THRESHOLD_TEST)]['R']\n",
    "c = next(colors)\n",
    "p2.circle(cvss_exp_r, cvss_exp_p, line_width=10,color=c,legend_label=r'Baseline: CVSSv3 Exploitability' )    \n",
    "p2.line(cvss_exp_r, cvss_exp_p, line_width=5,color=c,legend_label=r'Baseline: CVSSv3 Exploitability' )    \n",
    "\n",
    "\n",
    "show(p1)\n",
    "show(p2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ba84ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
